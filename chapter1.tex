
\nextchapter{Introduction to Optimizations}

\nextsubchapter{Definition of an optimization problem}

\begin{Definition}
An \textit{optimization problem} has the form

\begin{equation}\label{eq:optimization_problem}
	\begin{array}{rl}
	\text{minimize} & f_0(x) \\
	\text{subject to} & f_i(x)\le b_i,\quad i=1,\ldots,m
	\end{array}
\end{equation}

where:

\begin{itemize}
	\item $x=(x_1,\ldots,x_n)\in\mathbb R^n$ is the \textit{optimization variable};
	\item $f_0:\mathbb R^n\to\mathbb R$ is the \textit{objective function};
	\item $f_i:\mathbb R^n\to \mathbb R$, $i=1,\ldots,m$ are the \textit{constraint functions};
	\item $b_1,...,b_m$ are the limits (bounds) for the constraints.
\end{itemize}

\end{Definition}

\begin{Fact}
You can understand (\ref{eq:optimization_problem}) as an abstraction of choosing a vector in $\Reals^n$ from a set of candidate choices. In this case, constraints $f_i(x)\le bi$ are ``requirements'' or ``specifications'' while $f_0(x)$ is the cost of choosing $x$.

\begin{itemize}
\item If $f_0(x)$ is the cost, then $-f_0(x)$ is then the ``utility''
\end{itemize}
\end{Fact}

\begin{Definition}
A vector $x^*$ is \textit{optimal} or is a \textit{solution} if

\begin{equation*}
	\forall z\in\mathbb R^n\text{ satisfying the constraints,}\quad f_0(x^*)\le f_0(z).
\end{equation*}
\end{Definition}

\nextsubchapter{Classes of optimization problems}

Optimization problems are bundled into classes based on the form of their objective ($f_0$) and constraint ($f_i$) functions. Here are some below.

\begin{Fact}
	Note: in optimization jargon, a "program" is an alias for an "optimization problem".
\end{Fact}

\smalltitle{Linear program}

A linear program is (\ref{eq:optimization_problem}) where $f_0,...,f_m$ are linear functions. Recall:

\begin{Definition}
Function $f(x):\Reals^n\to\Reals$ is linear $\Leftrightarrow$
\begin{equation*}
\forall x,y\in\mathbb R^n,\,\,\forall\alpha\in\Reals,\quad f(\alpha x+\beta y)=\alpha f(x)+\beta f(y)
\end{equation*}
\end{Definition}

\smalltitle{Nonlinear program}

An optimization problem is nonlinear if it is not linear. Of course, there are many classes of nonlinear programs. A general nonlinear program can be applied to basically any optimization problem, however:

\begin{itemize}
	\item It may take a very long time to solve;
	\item It may not find the (global) solution;
	\item It may be very difficult to solve!
\end{itemize}

\smalltitle{Convex program}

...and convex optimization is one of them!

\begin{Definition}
	A \textbf{convex optimization problem} is one in which $f_0,\ldots,f_m$ are convex:
	$f(x):\mathbb R^n\to\mathbb R$ is convex if and only if $$
	\forall x,y\in\mathbb R^n, \forall \alpha,\beta\in\mathbb R_{+}\text{ s.t. }\alpha+\beta=1\quad\Rightarrow\quad f(\alpha x+\beta y)\le \alpha f(x)+\beta f(y).
	$$
\end{Definition}

Unlike general nonlinear programs, convex (and linear) programs can be solved \textit{efficiently} and \textit{reliably}, making them ideally suited for real world (and even high stakes) applications!

In fact, a convex program is a common parent of two well know, reliable and efficient optimization classes: least squares and linear programs.

\smalltitle{Least-Squared Problems}

\begin{Definition}
	\begin{equation*}
		\text{minimize}\,\, \fnorm{Ax-b}{2}^2
	\end{equation*}
\end{Definition}

Least-squares programming is a \textit{mature technology}: least-squares problems can be solved efficiently and reliably. Many software packages exist to do this.

\begin{Fact}
	Least-squares properties:
	\begin{itemize}
		\item The analytical solution for $A\in\mathbb R^{k\times n}$ tall (i.e. $k>n$) and full rank (i.e. full column rank, $\text{null}(A)=\emptyset$): $x^*=(A^TA)^{-1}A^Tb$;
		\begin{itemize}
			\item NB: good solvers don't do literally this operation, though!
		\end{itemize}
		\item Compute time $\propto n^2k$.
	\end{itemize}
\end{Fact}

How to know if your problem is a least-squares problem? \textbf{Simple, just one question}:

\begin{itemize}
	\item Q: is the objective function the 2-norm squared of an affine function of $x$ (and you have no constraints)?
	\begin{itemize}
		\item A: Yes: it's a least-squares problem!
		\item A: No: it's not a least squares problem!
	\end{itemize}
\end{itemize}

\smalltitle{Linear Programming}

\begin{Definition}
	$$
	\begin{array}{rl}
	\text{minimize} & c^Tx \\
	\text{subject to} & a_i^Tx\le b_i,\quad i=1,\ldots,m
	\end{array}
	$$
\end{Definition}


Linear programming is, like least squares, a mature technology. Existing algorithms are (almost) as reliable as least squares.

\begin{Fact}
	Some linear programming properties:
	
	\begin{itemize}
		\item No analytical solution (except for trivial cases)!
		\item Computation time $\propto n^2m$ if $m\ge n$ (more constraints than optimization variables);
		\begin{itemize}
			\item Less with structure;
			\item NB: this is the same computational time as least-squares!
			\item It is good news that it's $n^2m$ and not $m^2n$ (since generally there can be very many constraints on a relatively small optimization variable)
		\end{itemize}
	\end{itemize}
\end{Fact}






